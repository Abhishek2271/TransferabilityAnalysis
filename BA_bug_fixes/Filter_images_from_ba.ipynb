{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, r'C:\\Users\\sab\\Downloads\\AI Testing\\Source\\Dorefanet\\tensorpack\\FullPrecisionModels')\n",
    "import save_restore_images\n",
    "import visualize_data\n",
    "from tensorpack.dataflow.dataset.mnist import Mnist\n",
    "import norm_distances_l2_ba as lpnorm \n",
    "from DataSets.mnist import GetMnist\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from matplotlib import pyplot\n",
    "import scipy.stats as stats\n",
    "from DataSets.cifar import Cifar10, get_cifar10_data, getaugmenteddata_with_all_images, _parse_meta\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = r\"C:\\Users\\sab\\Downloads\\AI Testing\\Source\\Dorefanet\\tensorpack\\FullPrecisionModels\\logs\\trained_images\\BA\\NEW CODE\\MNIST\\MODEL_B_REGULARIZED_ONLY_CH_32\\15_ITT\\mnist-16,16,32\"\n",
    "image_name = r\"mnist_conv_adv_pre-16,16,32--run-2.npz\"\n",
    "\n",
    "\n",
    "ba_logger = logging.getLogger(\"conversion_logger_{}\".format(image_folder + image_name.split(\".\")[0]))\n",
    "ba_logger.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler(os.path.join(image_folder, \"conversion_log_{}.log\".format(image_name.split(\".\")[0])))\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ba_logger.addHandler(fh)\n",
    "\n",
    "ba_logger.info(\"source file: {}\".format(image_folder + image_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter all images that did not converge. \n",
    "## Basic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The basic idea here is that since most of the images converged and only some were distorted beyond acceptance, we can assume that the data follows the \"Normal Distribution\". Meaning that most of the data points lie near the mean value while the completely distorted ones are outliers and lie far from mean value.\n",
    "\n",
    "2. We checked for 10, 20 and 50 iterations for 100, 100 and 2000 data points and the data does follow the Normal Distribution. This means that the images that are completely distorted lie far away from the mean of the data.  \n",
    "\n",
    "3. Thus, we first set a certain number of iterations and find adversarial samples as usual using __Boundary Attack Algorithm__ from the APP. This creates a .npz file containing the adversarial image, correct label and the image index (image index here is the index signifying image location (index) in the main db \"MNIST\" or \"CIFAR\" and not on the subsets).  \n",
    "\n",
    "4. We then use the saved npz file to find l2 distance (because algorithm uses l2 norm for distance calculation) of each image with its adversarial counter-part (the perturbed image being generated from the Boundary Attack Algorithm for a certain amount of iterations).  \n",
    "\n",
    "5. We then get an array of l2 distances (this array is in same sequence as the image_index).  \n",
    "\n",
    "6. We then convert all the l2 distance to standard normal distribution using stats.zscore function provided by the python's scipy library. Here, _\"conversion to standard normal distribution\"_ means that we have an array of l2 distances, we then use the zscore formula to get the zscore of each of the l2 distances. The Zscore, as we know, gives us how much a data point is away from from the mean in terms of standard deviation (basically how many standard deviations is there in the diff between the data point and the mean).  \n",
    "\n",
    "7. Since the data is stored sequentially, each l2 distance corresponds to the sequential image index  and each image index then corresponds to the corresponding image and labels.  \n",
    "\n",
    "8. We then group each l2 distance with corresponding image_index and form a tuple of values.  \n",
    "\n",
    "9. We then filter out (reject) all the image indexes that have z score more than 1 (we take 1 because the data we are operating on is quite small (2000) and it is better to remove highly distorted but still reconizable images rather than including convergence failed images). So we take everything on the left side whre z-score is less than 1. Everything on left side because z-score less than 0 is even better because this means data is smaller than mean (l2 is smaller than mean so less distortion)\n",
    "\n",
    "10. This gives us a set of image_indices that have l2 distances close to the mean value meaning only images that that are not very highly distorted.\n",
    "\n",
    "11. We then use the image index to filter the images and labels.\n",
    "\n",
    "12. We then save the image, labels and image index to a new npz file which is then used for analysis.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_image = save_restore_images.save_or_load_image__npz(\"load\", image_folder + \"\\\\\"+ image_name)\n",
    "adv_images = npz_image.images\n",
    "labels = npz_image.labels\n",
    "img_index = npz_image.image_index\n",
    "\n",
    "ba_logger.info(\"########################## check data distribution###########################\") \n",
    "\n",
    "print(\"Image Count: \", adv_images.shape)\n",
    "ba_logger.info(\"Image Count: {}\".format(adv_images.shape))\n",
    "print(\"Labels Count: \", labels.shape)\n",
    "ba_logger.info(\"Labels Count: {} \".format(labels.shape))\n",
    "print(\"Index Count: \", img_index.shape)\n",
    "ba_logger.info(\"Index Count: {}\".format(img_index.shape))\n",
    "\n",
    "l2_f, l2_dist, image_index = lpnorm.get_lp_norm_distances(adv_images, img_index)\n",
    "print(\"Number of image_index returned from function:  {}\".format(image_index.shape))\n",
    "ba_logger.info(\"Number of image_index returned from function:  {}\".format(image_index.shape))\n",
    "\n",
    "print(np.array_equal(img_index, image_index))\n",
    "l2 = float(\"{0:.4f}\".format(l2_f))\n",
    "print(\"Average l2 distance of the data points:\", l2)\n",
    "ba_logger.info(\"Average l2 distance of the data points:  {}\".format(l2)) \n",
    "\n",
    "\n",
    "z_scores = stats.zscore(l2_dist)\n",
    "visualize_data.plot_images(adv_images)\n",
    "#enumerate each z_score. This makes it easy for debugging\n",
    "#j = 0\n",
    "#for zscore in z_scores:  \n",
    "#    print(\"{} : {} \".format(j, zscore))\n",
    "#    j = j +1\n",
    "\n",
    "#j = 0\n",
    "#for l2 in l2_dist:  \n",
    "#    print(\"{} : {} \".format(j, l2))\n",
    "#    j = j +1\n",
    "\n",
    "#View histogram of l2 distance to view overall distribution of data\n",
    "#pyplot.hist(l2_dist)\n",
    "#view histogram after converting to standard normal distribution\n",
    "#Both should give same plot but \"pyplot.hist(z_scores)\" will be defined in terms of z scores while the first one just gives distribution of data\n",
    "n, bins, pack = pyplot.hist(z_scores)\n",
    "print(\"n:\", n)\n",
    "print(\"bins:\", bins)\n",
    "ba_logger.info(\"n :  {}\".format(n)) \n",
    "ba_logger.info(\"bins :  {}\".format(bins)) \n",
    "\n",
    "print(image_index)\n",
    "\n",
    "ba_logger.info(\"########################## data distribution check ended ###########################\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform filteration of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_logger.info(\"########################## Filtering images ###########################\") \n",
    "\n",
    "npz_image = save_restore_images.save_or_load_image__npz(\"load\", image_folder + \"\\\\\"+ image_name)\n",
    "\n",
    "adv_images = npz_image.images\n",
    "labels = npz_image.labels\n",
    "img_index = npz_image.image_index\n",
    "\n",
    "print(\"Image Count: \", adv_images.shape)\n",
    "print(\"Labels Count: \", labels.shape)\n",
    "print(\"Index Count: \", img_index.shape)\n",
    "ba_logger.info(\"Image Count:  {} \".format(adv_images.shape))\n",
    "ba_logger.info(\"Labels Count:  {} \".format(labels.shape))\n",
    "ba_logger.info(\"Index Count:   {}\".format(img_index.shape)) \n",
    "\n",
    "#get an array containig l2 distances. The function \"get_lp_norm_distances\" is same as norm_distances.py but with l2_distance array\n",
    "l2_f, l2_dist, image_index = lpnorm.get_lp_norm_distances(adv_images, img_index)\n",
    "\n",
    "#check how many image index are returned and how many elements are there in l2_distance. BOTH SHOULD BE SAME\n",
    "print(\"Number of image_index returned from function: \", image_index.shape)\n",
    "print(np.array_equal(img_index, image_index))\n",
    "l2 = float(\"{0:.4f}\".format(l2_f))\n",
    "print(\"Total number of l2 distances: \", len(l2_dist))\n",
    "ba_logger.info(\"Total number of l2 distances: {}\".format(len(l2_dist)))\n",
    "\n",
    "\n",
    "#print average l2 distance of the data points\n",
    "print(\"Average l2 distance of the data points:\", l2)\n",
    "ba_logger.info(\"Average l2 distance of the data points: {}\".format(l2))\n",
    "\n",
    "\n",
    "#compute z scores\n",
    "z_scores = stats.zscore(l2_dist)\n",
    "\n",
    "#Make a tuple of Z score and the image index. So each image is then paired with its corresponding Z score.\n",
    "# eg (983, -3.144). This basically means that the adversarial image of the 983th image in the MNIST dataset is below the average l2 distance.\n",
    "# 3 standard deviations away from mean (mean is zero in standard normal distrib) \n",
    "index_zscore = list(zip(image_index, z_scores))\n",
    "\n",
    "\n",
    "#Get z-scores that need to be removed (just for debugging, this step)\n",
    "filtered_zscore = z_scores[z_scores<1]\n",
    "print(\"Number of allowed z_scores: \", len(filtered_zscore))\n",
    "ba_logger.info(\"Number of allowed z_scores:  {}\".format(len(filtered_zscore)))\n",
    "\n",
    "#Get tuples that have z score less than 1 i.e within 1 standard deviation away from mean (on right side). On left side (negative) \n",
    "# it is better that l2 distance is less than average because this means the image is more closer to the original image and thus \n",
    "# algorithm is working for those images. \n",
    "# Note: since within 1 standard deviation, in a standard normal distribution, there are usually 68% data point (non skewed) so may\n",
    "# be we get similar to that number (higher than that actually because we take all from left side of the curve). We have to take more \n",
    "# data points (in the initial npz) to get 2000 data points after filter\n",
    "\n",
    "# Filtered_indices will have no tuples with z score >=1 (this step is also only for debug)\n",
    "# Filtered indices is a tuple of (filtered_image_index, Z-score)\n",
    "filtered_indices = [i for i in index_zscore if i[1]<1]\n",
    "#print(\"Filtered data: \",filtered_indices)\n",
    "print(\"Filtered z_scores and index count:\", len(filtered_indices))\n",
    "ba_logger.info(\"Filtered z_scores and index count:  {}\".format(len(filtered_indices)))\n",
    "\n",
    "#from the tuples get only the image index that have z score < 1\n",
    "selected_image_index = [i[0]  for i in index_zscore if i[1]<1]\n",
    "#selected_image_index = [i for i in index_zscore if i[1]>=1] #to check for only distorted images\n",
    "#convert to np array\n",
    "selected_image_index = np.array(selected_image_index)\n",
    "#note: the filtered index count and the filtered z-scores count should be same\n",
    "print(\"Filtered index count:\", len(filtered_indices))\n",
    "ba_logger.info(\"Filtered index count:  {}\".format(len(filtered_indices)))\n",
    "\n",
    "img_valid_z_score_img = []      #list containing all filtered images\n",
    "img_valid_z_score_label = []    #list containing corresponding labels of filtered images\n",
    "img_valid_z_score_index = []    #list containing corresponding image index (same as selected_image_index)\n",
    "\n",
    "#get images and labels that are filtered according to the image index obtained\n",
    "for select_index in selected_image_index:\n",
    "    z_score_index = np.where(image_index == select_index)\n",
    "    _image = adv_images[z_score_index][0]\n",
    "    img_valid_z_score_img.append(_image)\n",
    "    _label = labels[z_score_index][0]\n",
    "    img_valid_z_score_label.append(_label)\n",
    "    _index = img_index[z_score_index][0]\n",
    "    img_valid_z_score_index.append(_index)\n",
    "\n",
    "#convert to np array\n",
    "img_valid_z_score_img = np.array(img_valid_z_score_img)\n",
    "img_valid_z_score_label = np.array(img_valid_z_score_label)\n",
    "img_valid_z_score_index = np.array(img_valid_z_score_index)\n",
    "\n",
    "print(\"number of filtered images\", img_valid_z_score_img.shape)\n",
    "print(\"number of filtered labels\", img_valid_z_score_label.shape)\n",
    "print(\"number of indixes\", img_valid_z_score_index.shape)\n",
    "\n",
    "ba_logger.info(\"number of filtered images  {}\".format(img_valid_z_score_img.shape))\n",
    "ba_logger.info(\"number of filtered labels  {}\".format(img_valid_z_score_label.shape))\n",
    "ba_logger.info(\"number of indixes  {}\".format(img_valid_z_score_index.shape))\n",
    "\n",
    "\n",
    "\n",
    "print(len(img_valid_z_score_index)) \n",
    "print(len(selected_image_index))\n",
    "print(np.array_equal(img_valid_z_score_index, selected_image_index))\n",
    "\n",
    "ba_logger.info(\"is the computed index count matches with the index count added later:  {} \".format(np.array_equal(img_valid_z_score_index, selected_image_index)))\n",
    "\n",
    "visualize_data.plot_images(img_valid_z_score_img)\n",
    "\n",
    "#print(len(filtered_indices))\n",
    "#print(filtered_indices)\n",
    "#print(image_index)\n",
    "\n",
    "# Save to a new npz file\n",
    "\n",
    "ba_logger.info(\"########################## images filtered ###########################\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the updated l2 distance after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get lp_distances of the filtered images\n",
    "updated_l2_f, updated_l2_dist, updated_image_index = lpnorm.get_lp_norm_distances(img_valid_z_score_img, img_valid_z_score_index)\n",
    "updated_l2 = float(\"{0:.4f}\".format(updated_l2_f))\n",
    "print(\"Total number of --updated-- l2 distances: \", len(updated_l2_dist))\n",
    "ba_logger.info(\"Total number of --updated-- l2 distances: {}\".format(len(updated_l2_dist)))\n",
    "\n",
    "#print average l2 distance of the data points\n",
    "print(\"Average --updated after filter-- l2 distance of the data points:\", updated_l2)\n",
    "ba_logger.info(\"Average --updated after filter-- l2 distance of the data points: {}\".format(updated_l2))\n",
    "print(np.array_equal(updated_image_index, img_valid_z_score_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual images for QC\n",
    "#print(img_valid_z_score_label)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(image_index[96])\n",
    "#print(image_index)\n",
    "\n",
    "Mnist_Data_Dir = r\"C:\\Users\\sab\\Downloads\\AI Testing\\_Tools\\DataSets\\MNIST\\Data\"\n",
    "data_test = GetMnist('test', dir=Mnist_Data_Dir)\n",
    "\n",
    "visualize_data.plot_image(img_valid_z_score_img[96])\n",
    "print(img_valid_z_score_label[96])\n",
    "print(img_valid_z_score_index[96])\n",
    "visualize_data.plot_image(data_test.images[3412])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check individual indices for QC\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(selected_image_index)\n",
    "#print(npz_image.image_index)\n",
    "\n",
    "ds = get_cifar10_data(\"test\",dir= r\"C:\\Users\\sab\\Downloads\\AI Testing\\_Tools\\DataSets\\CIFAR10\")\n",
    "data_set = getaugmenteddata_with_all_images(ds)\n",
    "\n",
    "visualize_data.plot_image(npz_image.images[4])\n",
    "print(npz_image.labels[4])\n",
    "print(npz_image.image_index[4])\n",
    "\n",
    "visualize_data.plot_image(img_valid_z_score_img[96])\n",
    "print(img_valid_z_score_label[96])\n",
    "print(img_valid_z_score_index[96])\n",
    "\n",
    "visualize_data.plot_image(data_set.images[9407])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_logger.info(\"########################## saving file ###########################\") \n",
    "save_file_name = image_name.split(\".\")[0] + \"__filtered.npz\"\n",
    "save_restore_images.save_or_load_image__npz(\n",
    "    \"save\", \n",
    "    image_folder + \"\\\\\"+ save_file_name, \n",
    "    image= img_valid_z_score_img,    \n",
    "    labels= img_valid_z_score_label , \n",
    "    image_index= img_valid_z_score_index)\n",
    "ba_logger.info(\"saved to file:  {}\".format(save_file_name))\n",
    "\n",
    "ba_logger.info(\"########################## file saved ###########################\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST THE SAVED RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mnist_Data_Dir = r\"C:\\Users\\sab\\Downloads\\AI Testing\\_Tools\\DataSets\\MNIST\\Data\"\n",
    "#data_test = GetMnist('test', dir=Mnist_Data_Dir)\n",
    "\n",
    "ds = get_cifar10_data(\"test\",dir= r\"C:\\Users\\sab\\Downloads\\AI Testing\\_Tools\\DataSets\\CIFAR10\")\n",
    "data_set = getaugmenteddata_with_all_images(ds)\n",
    "\n",
    "f_npz_image = save_restore_images.save_or_load_image__npz(\"load\", \n",
    "r\"C:\\Users\\sab\\Downloads\\AI Testing\\Source\\Dorefanet\\tensorpack\\FullPrecisionModels\\logs\\trained_images\\BA\\NEW CODE\\CIFAR\\RESNET_5\\12_ITT_3000_SAMPLES\\cifar10-16,16,32\\mnist_conv_adv_pre-16,16,32--run-2__filtered.npz\")\n",
    "print(f_npz_image.images.shape)\n",
    "print(f_npz_image.labels.shape)\n",
    "print(f_npz_image.image_index.shape)\n",
    "f_images = f_npz_image.images\n",
    "f_labels = f_npz_image.labels\n",
    "f_image_index = f_npz_image.image_index\n",
    "\n",
    "f_npz_image1 = save_restore_images.save_or_load_image__npz(\"load\", \n",
    "r\"C:\\Users\\sab\\Downloads\\AI Testing\\Source\\Dorefanet\\tensorpack\\FullPrecisionModels\\logs\\trained_images\\BA\\NEW CODE\\CIFAR\\RESNET_5\\12_ITT_3000_SAMPLES\\cifar10-16,16,32\\mnist_conv_adv_pre-16,16,32--run-2__filtered.npz\")\n",
    "\n",
    "f_images1 = f_npz_image1.images\n",
    "f_labels1 = f_npz_image1.labels\n",
    "f_image_index1 = f_npz_image1.image_index\n",
    "\n",
    "visualize_data.plot_image(f_images1[281])\n",
    "\n",
    "print(f_labels1[281])\n",
    "print(f_image_index1[281])\n",
    "\n",
    "#print(npz_image.image_index[94])\n",
    "#visualize_data.plot_image(npz_image.images[0])\n",
    "#print(f_image_index)\n",
    "#print(np.array_equal(f_labels, img_valid_z_score_label))\n",
    "#print(np.array_equal(f_images, img_valid_z_score_img))\n",
    "#print(np.array_equal(f_image_index, selected_image_index))\n",
    "visualize_data.plot_image(data_set.images[8334])\n",
    "print(data_set.labels[8334])\n",
    "#visualize_data.plot_images(f_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "l2 = [1,2,2,2,2,2,2,2,2,2,2,2,2,3,4]\n",
    "l3 = np.array([2,4])\n",
    "pyplot.hist(l2)\n",
    "print(l2)\n",
    "l2 = np.array(l2)\n",
    "ind = [0,1]\n",
    "print(l2[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test (inp, i1=\"1\",i2=2):\n",
    "    print(inp,i1,i2)\n",
    "\n",
    "test(\"input\", i2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "\n",
    "print(art.__version__)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04635d289a519a1410467dd0afb0db42f9184808881ca68b2eb5a687a20a5a94"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
